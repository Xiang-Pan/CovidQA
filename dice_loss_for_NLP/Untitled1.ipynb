{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b446397-ffd4-438d-96fe-2c7cbc7afaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e285e9-7922-41d9-80a3-448bb255d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6771829a-e39b-4b0f-bd8e-5c2710090067",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"location entities are limited to geographical entities such as geographical areas and landmasses, mountains, bodies of water, and geological formations. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cde449-550c-424f-8eb2-0574e55887e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"from the wounds of a bitterly fought presidential contest in history , the president - elect has his work cut out for him .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a251c2d-11ef-4743-9efe-71e1a3806691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7761,\n",
       " 5,\n",
       " 9308,\n",
       " 9,\n",
       " 10,\n",
       " 31337,\n",
       " 4951,\n",
       " 1939,\n",
       " 3096,\n",
       " 11,\n",
       " 750,\n",
       " 2156,\n",
       " 5,\n",
       " 394,\n",
       " 111,\n",
       " 10371,\n",
       " 34,\n",
       " 39,\n",
       " 173,\n",
       " 847,\n",
       " 66,\n",
       " 13,\n",
       " 123,\n",
       " 479]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499c82d4-b767-458a-8aa2-ac846601c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = tokenizer(query,add_special_tokens= False)['input_ids']\n",
    "context_ids = tokenizer(context,add_special_tokens= False)['input_ids']\n",
    "ans_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2968073c-e584-48f2-94f4-c173ff4a90e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44538,\n",
       " 8866,\n",
       " 32,\n",
       " 1804,\n",
       " 7,\n",
       " 2166,\n",
       " 2110,\n",
       " 215,\n",
       " 25,\n",
       " 9571,\n",
       " 13,\n",
       " 7217,\n",
       " 28428,\n",
       " 215,\n",
       " 25,\n",
       " 5013,\n",
       " 6,\n",
       " 3701,\n",
       " 8,\n",
       " 1751,\n",
       " 42289,\n",
       " 4]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e59b91c5-aa53-44e3-981e-d4c6d96943a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7761,\n",
       " 5,\n",
       " 9308,\n",
       " 9,\n",
       " 10,\n",
       " 31337,\n",
       " 4951,\n",
       " 1939,\n",
       " 3096,\n",
       " 11,\n",
       " 750,\n",
       " 2156,\n",
       " 5,\n",
       " 394,\n",
       " 111,\n",
       " 10371,\n",
       " 34,\n",
       " 39,\n",
       " 173,\n",
       " 847,\n",
       " 66,\n",
       " 13,\n",
       " 123,\n",
       " 479]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b90d7a9-8f82-4232-85f7-a74640cccffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_context_ids = tokenizer.build_inputs_with_special_tokens(query_ids,context_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94dff22b-9b40-4dcf-a680-c581af7dfdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>weapon entities are limited to physical devices such as instruments for physically harming such as guns, arms and gunpowder.</s></s>from the wounds of a bitterly fought presidential contest in history , the president - elect has his work cut out for him .</s>'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(query_context_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342e791-cb79-4d0d-8ec5-4542dbc8c4a7",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e16010-903e-4918-9bb9-e2f6d7885787",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f7141bcb0b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Who was Jim Henson?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Jim Henson was a nice puppet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m             )\n\u001b[1;32m   2216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2218\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2285\u001b[0m         )\n\u001b[1;32m   2286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2287\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2288\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         return self.prepare_for_model(\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mpair_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecond_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   2751\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2753\u001b[0;31m         batch_outputs = BatchEncoding(\n\u001b[0m\u001b[1;32m   2754\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTENSORFLOW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m    585\u001b[0m                     \u001b[0;34m\"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                 )\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to convert output to TensorFlow tensors format, TensorFlow is not installed."
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "# model = TFRobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_dict = tokenizer(question, text, return_tensors='pt')\n",
    "outputs = model(input_dict)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b6645cc-e042-4901-9cb0-2e07eb6eba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tokenizer(question, text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d020d23-0e48-459f-aa3f-b5f6e66b9712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 12375,    21,  2488,   289, 13919,   116,     2,     2, 24021,\n",
       "           289, 13919,    21,    10,  2579, 29771,     2]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f9f98c67-a461-4fac-9199-652705cc8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a9c4b0d-b571-418c-bdcb-5613639212ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_token_to_id(\"Jim Henson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f9f49d1-06c7-4936-a948-1854fe4c4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2eb7c0d-7113-43d4-877c-e2668406ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [0, 1899, 25032, 559, 8866, 32, 20456, 3806, 6533, 30, 559, 8, 50, 592, 1134, 215, 25, 749, 6, 3949, 6, 3806, 6, 1947, 6, 982, 6, 168, 8, 63, 82, 4, 2, 2, 1000, 179, 15363, 491, 3131, 2156, 7137, 2156, 830, 1105, 620, 2156, 30, 4439, 344, 906, 1657, 28279, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed39aca5-9f3a-46bf-9a2e-6873b6966aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "3609 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cc708f91ef77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3609\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 3609 is not in list"
     ]
    }
   ],
   "source": [
    "a.index(3609)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c69e28-7b4e-4b18-ba86-a9c2c092c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = tokenizer.convert_ids_to_tokens(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6aa0f2c0-cca3-4e3d-a200-9dd5dd9aa6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([0,7137, 1437])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8579373a-5883-4f1f-8865-88aaea55414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"location entities are limited to geographical entities such as geographical areas and landmasses, mountains, bodies of water, and geological formations. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fc456d9-62be-43bf-ba73-ac0c3306330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>weapon entities are limited to physical devices such as instruments for physically harming such as guns, arms and gunpowder.</s></s>from the wounds of a bitterly fought presidential contest in history , the president - elect has his work cut out for him .</s>'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(query_context_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b578e5fc-b3f3-4ef3-bd86-7f4365e61277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2488, 289, 13919, 1437, 2]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" Jim Henson \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44499f68-4742-494b-8250-ae15e7e1b97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95a1306e-f6b2-42c9-aaf4-7413a04aefc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä Shanghai']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0,7137, 1437])[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a75a6-bd60-4d35-beb1-da45082559d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
