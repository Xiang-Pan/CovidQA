{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7a9b6b-6afe-491c-b253-37eea5543520",
   "metadata": {},
   "source": [
    "# Mark Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fd50ef-72c2-4f5e-8ba6-27a97e46cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f77efd5c-011c-4d34-8af2-8182b59def32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"./datasets/covidqa/dev-v1.1.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16a22265-8e3c-4070-9455-22ff2a73d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"./context_dev.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cc5d7f4-11d6-4df9-9f35-a351d00f7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset[\"data\"]:\n",
    "    for para in data['paragraphs']:\n",
    "        context = para[\"context\"]\n",
    "        out_file.write(context)\n",
    "        out_file.write(\"\\n\")\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8284998d-e2bf-4e57-afac-1c60cffc8d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'qas': [{'question': 'Is the origin and epidemiology of the 1918 swine flu (Spanish Influenza) known?',\n",
       "     'id': '1075',\n",
       "     'answers': [{'text': 'ongoing studies to map Virulence\\nfactors are yielding interesting results. The 1918 sequence\\ndata, however, leave unanswered questions about the ori-\\ngin of the Virus (19) and about the epidemiology of the\\npandemic.',\n",
       "       'answer_start': 5741}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'How dangerous are the modern H1N1 (swine flu) and the H3N2 (Influenza A) viruses compared  to the 1918 H1N1 (swine flu Spanish Influenza)  viruses?',\n",
       "     'id': '1072',\n",
       "     'answers': [{'text': 'the human H1N1 and H3N2 lin-\\neages have both been associated with substantially lower\\nrates ofillness and death than the virus of 1918. In fact, cur-\\nrent H1N1 death rates are even lower than those for H3N2\\nlineage strains (prevalent from 1968 until the present).',\n",
       "       'answer_start': 3781}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Is there a difference in the pathologic feature and course of disease between modern influenza pandemics and the 1918 swine flu pandemic?',\n",
       "     'id': '1114',\n",
       "     'answers': [{'text': ' the 1918\\npandemic was different in degree, but not in kind, from\\nprevious and subsequent pandemics. Despite the extraordi-\\nnary number of global deaths, most inﬂuenza cases in\\n1918 (>95% in most locales in industrialized nations) were\\nmild and essentially indistinguishable from inﬂuenza cases\\ntoday. ',\n",
       "       'answer_start': 26867}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Why the human influenza viruses do not disappear after herd immunity is developed?',\n",
       "     'id': '1096',\n",
       "     'answers': [{'text': 'The occurrence, and to some extent the severity, of recur-\\nrent annual outbreaks, are driven by Viral antigenic drift,\\nwith an antigenic variant Virus emerging to become domi-\\nnant',\n",
       "       'answer_start': 11043}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Which virus samples  from the 1918 swine flu pandemic have been identified?',\n",
       "     'id': '1105',\n",
       "     'answers': [{'text': 'pandemic Virus samples we have\\nyet identiﬁed are from second-wave patients',\n",
       "       'answer_start': 14082}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'How many people were infected during the 1918 Spanish Influenza epidemic?',\n",
       "     'id': '1058',\n",
       "     'answers': [{'text': 'An estimated one third of the world’s population (or\\nz500 million persons) were infected and had clinical-\\nly apparent illnesses',\n",
       "       'answer_start': 985}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Is the geographical origin of the 1918 H1N1 swine flu known?',\n",
       "     'id': '1087',\n",
       "     'answers': [{'text': 'Confounding deﬁnite assignment of a geographic\\npoint of origin, the 1918 pandemic spread more or less\\nsimultaneously in 3 distinct waves during an z12-month\\nperiod in 191871919, in Europe, Asia, and North America\\n(the ﬁrst wave was best described in the United States in\\nMarch 1918)',\n",
       "       'answer_start': 6125}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Was the 1918 swine flu virus novel to humans are was it derived from older viruses?',\n",
       "     'id': '1107',\n",
       "     'answers': [{'text': 'Viral sequence data now suggest that the entire 1918\\nVirus was novel to humans in, or shortly before, 1918, and\\nthat it thus was not a reassortant Virus produced from old\\nexisting strains that acquired 1 or more new genes',\n",
       "       'answer_start': 14934}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What descendant  lineages of the swine flu (Spanish Influenza) virus were identified in 2006?',\n",
       "     'id': '1068',\n",
       "     'answers': [{'text': ' 2 major descendant lineages of the 1918\\nH1N1 Virus, as well as 2 additional reassortant lineages,\\npersist naturally: a human epidemic/endemic H1N1 line-\\nage, a porcine enzootic H1N1 lineage (so-called classic\\nswine ﬂu), and the reassorted human H3N2 Virus lineage,\\nwhich like the human H1N1 Virus, has led to a porcine\\nH3N2 lineage.',\n",
       "       'answer_start': 3271}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What theory provides partial explanation for the age-specific profile of the death rate in the 1918 swine flu pandemic?',\n",
       "     'id': '1113',\n",
       "     'answers': [{'text': ' the 1918 Virus had an intrinsically high Virulence, tem-\\npered only in those patients who had been born before\\n1889, e.g., because of exposure to a then-circulating Virus\\ncapable of providing partial immunoprotection against the\\n1918 Virus strain only in persons old enough (>35 years) to\\nhave been infected during that prior era ',\n",
       "       'answer_start': 24099}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What was the death toll in the 1918-1919 Spanish Influenza epidemic?',\n",
       "     'id': '1060',\n",
       "     'answers': [{'text': 'Total deaths were estimated at\\nz50 million (577) and were arguably as high as 100 mil-\\nlion ',\n",
       "       'answer_start': 1284}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'When was it  determined that the 1918  pandemic was caused by the H1N1 Influenza virus?',\n",
       "     'id': '1064',\n",
       "     'answers': [{'text': 'That question did not begin to be resolved until the 1930s,\\nwhen closely related inﬂuenza Viruses (now known to be\\nH1N1 Viruses) were isolated, ﬁrst from pigs and shortly\\nthereafter from humans. Seroepidemiologic studies soon\\nlinked both of these viruses to the 1918 pandemic',\n",
       "       'answer_start': 2355}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'When did the Swine Flu (Spanish Influenza) virus reappear in humans?',\n",
       "     'id': '1066',\n",
       "     'answers': [{'text': ' But in 1977, human H1N1 Viruses\\nsuddenly “reemerged” from a laboratory freezer (9). They\\ncontinue to circulate endemically and epidemically.',\n",
       "       'answer_start': 3115}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Why is the Spanish Influenza virus the Mother of the modern influenza viruses?',\n",
       "     'id': '1063',\n",
       "     'answers': [{'text': 'The latter are composed of key\\ngenes from the 1918 Virus, updated by subsequently-incor—\\nporated avian inﬂuenza genes that code for novel surface\\n\\n \\n\\n*Armed Forces Institute of Pathology, Rockville, Maryland, USA;\\nand TNational Institutes of Health, Bethesda, Maryland, USA\\n\\nproteins, making the 1918 Virus indeed the “mother” of all\\npandemics.',\n",
       "       'answer_start': 1729}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What was the age profile of mortality in the 1918 swine flu?',\n",
       "     'id': '1110',\n",
       "     'answers': [{'text': 'age-speciﬁc death rates in the\\n1918 pandemic exhibited a distinct pattern that has not been\\ndocumented before or since: a “W—shaped” curve, similar to\\nthe familiar U-shaped curve but with the addition of a third\\n(middle) distinct peak of deaths in young adults z20410\\nyears of age',\n",
       "       'answer_start': 21925}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Which age group was most susceptible to die during the 1918 swine flu pandemic?',\n",
       "     'id': '1111',\n",
       "     'answers': [{'text': 'Persons 65 years of age in 1918 had a dispro-\\nportionately high inﬂuenza incidence',\n",
       "       'answer_start': 23095}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What was the case fatality rate in the 1918 Spanish Influenza epidemic?',\n",
       "     'id': '1059',\n",
       "     'answers': [{'text': 'Case-\\nfatality rates were >2.5%, compared to <0.1% in other\\ninﬂuenza pandemics',\n",
       "       'answer_start': 1198}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Could the 1918 swine flu virus been controlled  by modern day drugs or vaccines?',\n",
       "     'id': '1117',\n",
       "     'answers': [{'text': 'the 1918 and 1918-like Viruses would be\\nas sensitive as other typical Virus strains to the Food and\\nDrug Administrationiapproved antiinﬂuenza drugs riman-\\ntadine and oseltamivir.',\n",
       "       'answer_start': 27291}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Are the modern descendant influenza viruses as dangerous as the 1918 parent swine flu (Spanish Influenza) H1N1 virus?',\n",
       "     'id': '1070',\n",
       "     'answers': [{'text': 'None of these Viral descendants, however,\\napproaches the pathogenicity of the 1918 parent Virus.',\n",
       "       'answer_start': 3605}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What was the primary difference between the first wave and the 2nd and 3rd wave of the 1918-1919 swine flu pandemic?',\n",
       "     'id': '1094',\n",
       "     'answers': [{'text': 'the much higher fre-\\nquency of complicated, severe, and fatal cases in the last 2\\nwaves.',\n",
       "       'answer_start': 10785}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What are the circumstances that promote the spread of influenza virus?',\n",
       "     'id': '1099',\n",
       "     'answers': [{'text': ' lower environ-\\nmental temperatures and human nasal temperatures (bene-\\nﬁcial to thermolabile Viruses such as inﬂuenza), optimal\\nhumidity, increased crowding indoors, and imperfect ven-\\ntilation due to closed windows and suboptimal airﬂow',\n",
       "       'answer_start': 11707}],\n",
       "     'is_impossible': False}],\n",
       "   'context': ' \\n\\n1918 Influenza: the Mother of All Pandemics\\n\\nJeffery K. Taubenberger\" and David M. Morens1-\\n\\nThe “Spanish\" influenza pandemic of 1918—1919,\\nwhich caused :50 million deaths worldwide, remains an\\nominous warning to public health. Many questions about its\\norigins, its unusual epidemiologic features, and the basis of\\nits pathogenicity remain unanswered. The public health\\nimplications of the pandemic therefore remain in doubt\\neven as we now grapple with the feared emergence of a\\npandemic caused by H5N1 or other virus. However, new\\ninformation about the 1918 virus is emerging, for example,\\nsequencing of the entire genome from archival autopsy tis-\\nsues. But, the viral genome alone is unlikely to provide\\nanswers to some critical questions. Understanding the\\n1918 pandemic and its implications for future pandemics\\nrequires careful experimentation and in-depth historical\\nanalysis.\\n\\n \\n\\n”Curiouser and curiouser/ ” criedAlice\\nLewis Carroll, Alice’s Adventures in Wonderland, 1865\\n\\nAn estimated one third of the world’s population (or\\nz500 million persons) were infected and had clinical-\\nly apparent illnesses (1,2) during the 191871919 inﬂuenza\\npandemic. The disease was exceptionally severe. Case-\\nfatality rates were >2.5%, compared to <0.1% in other\\ninﬂuenza pandemics (3,4). Total deaths were estimated at\\nz50 million (577) and were arguably as high as 100 mil-\\nlion (7).\\n\\nThe impact of this pandemic was not limited to\\n191871919. All inﬂuenza A pandemics since that time, and\\nindeed almost all cases of inﬂuenza A worldwide (except-\\ning human infections from avian Viruses such as H5N1 and\\nH7N7), have been caused by descendants of the 1918\\nVirus, including “drifted” H1N1 Viruses and reassorted\\nH2N2 and H3N2 Viruses. The latter are composed of key\\ngenes from the 1918 Virus, updated by subsequently-incor—\\nporated avian inﬂuenza genes that code for novel surface\\n\\n \\n\\n*Armed Forces Institute of Pathology, Rockville, Maryland, USA;\\nand TNational Institutes of Health, Bethesda, Maryland, USA\\n\\nproteins, making the 1918 Virus indeed the “mother” of all\\npandemics.\\n\\nIn 1918, the cause of human inﬂuenza and its links to\\navian and swine inﬂuenza were unknown. Despite clinical\\nand epidemiologic similarities to inﬂuenza pandemics of\\n1889, 1847, and even earlier, many questioned whether\\nsuch an explosively fatal disease could be inﬂuenza at all.\\nThat question did not begin to be resolved until the 1930s,\\nwhen closely related inﬂuenza Viruses (now known to be\\nH1N1 Viruses) were isolated, ﬁrst from pigs and shortly\\nthereafter from humans. Seroepidemiologic studies soon\\nlinked both of these viruses to the 1918 pandemic (8).\\nSubsequent research indicates that descendants of the 1918\\nVirus still persists enzootically in pigs. They probably also\\ncirculated continuously in humans, undergoing gradual\\nantigenic drift and causing annual epidemics, until the\\n1950s. With the appearance of a new H2N2 pandemic\\nstrain in 1957 (“Asian ﬂu”), the direct H1N1 Viral descen-\\ndants 0f the 1918 pandemic strain disappeared from human\\ncirculation entirely, although the related lineage persisted\\nenzootically in pigs. But in 1977, human H1N1 Viruses\\nsuddenly “reemerged” from a laboratory freezer (9). They\\ncontinue to circulate endemically and epidemically.\\n\\nThus in 2006, 2 major descendant lineages of the 1918\\nH1N1 Virus, as well as 2 additional reassortant lineages,\\npersist naturally: a human epidemic/endemic H1N1 line-\\nage, a porcine enzootic H1N1 lineage (so-called classic\\nswine ﬂu), and the reassorted human H3N2 Virus lineage,\\nwhich like the human H1N1 Virus, has led to a porcine\\nH3N2 lineage. None of these Viral descendants, however,\\napproaches the pathogenicity of the 1918 parent Virus.\\nApparently, the porcine H1N1 and H3N2 lineages uncom-\\nmonly infect humans, and the human H1N1 and H3N2 lin-\\neages have both been associated with substantially lower\\nrates ofillness and death than the virus of 1918. In fact, cur-\\nrent H1N1 death rates are even lower than those for H3N2\\nlineage strains (prevalent from 1968 until the present).\\nH1N1 Viruses descended from the 1918 strain, as well as \\nH3N2 Viruses, have now been cocirculating worldwide for\\n29 years and show little evidence of imminent extinction.\\n\\nTrying To Understand What Happened\\n\\nBy the early 1990s, 75 years of research had failed to\\nanswer a most basic question about the 1918 pandemic:\\nwhy was it so fatal? No Virus from 1918 had been isolated,\\nbut all of its apparent descendants caused substantially\\nmilder human disease. Moreover, examination of mortality\\ndata from the 1920s suggests that within a few years after\\n1918, inﬂuenza epidemics had settled into a pattern of\\nannual epidemicity associated with strain drifting and sub-\\nstantially lowered death rates. Did some critical Viral genet-\\nic event produce a 1918 Virus of remarkable pathogenicity\\nand then another critical genetic event occur soon after the\\n1918 pandemic to produce an attenuated H1N1 Virus?\\n\\nIn 1995, a scientiﬁc team identiﬁed archival inﬂuenza\\nautopsy materials collected in the autumn of 1918 and\\nbegan the slow process of sequencing small Viral RNA\\nfragments to determine the genomic structure of the\\ncausative inﬂuenza Virus (10). These efforts have now\\ndetermined the complete genomic sequence of 1 Virus and\\npartial sequences from 4 others. The primary data from the\\nabove studies (11717) and a number of reviews covering\\ndifferent aspects of the 1918 pandemic have recently been\\npublished ([8720) and conﬁrm that the 1918 Virus is the\\nlikely ancestor of all 4 of the human and swine H1N1 and\\nH3N2 lineages, as well as the “extinct” H2N2 lineage. No\\nknown mutations correlated with high pathogenicity in\\nother human or animal inﬂuenza Viruses have been found\\nin the 1918 genome, but ongoing studies to map Virulence\\nfactors are yielding interesting results. The 1918 sequence\\ndata, however, leave unanswered questions about the ori-\\ngin of the Virus (19) and about the epidemiology of the\\npandemic.\\n\\nWhen and Where Did the 1918 Inﬂuenza\\nPandemic Arise?\\n\\nBefore and after 1918, most inﬂuenza pandemics\\ndeveloped in Asia and spread from there to the rest of the\\nworld. Confounding deﬁnite assignment of a geographic\\npoint of origin, the 1918 pandemic spread more or less\\nsimultaneously in 3 distinct waves during an z12-month\\nperiod in 191871919, in Europe, Asia, and North America\\n(the ﬁrst wave was best described in the United States in\\nMarch 1918). Historical and epidemiologic data are inade-\\nquate to identify the geographic origin of the Virus (21),\\nand recent phylogenetic analysis of the 1918 Viral genome\\ndoes not place the Virus in any geographic context ([9).\\n\\nAlthough in 1918 inﬂuenza was not a nationally\\nreportable disease and diagnostic criteria for inﬂuenza and\\npneumonia were vague, death rates from inﬂuenza and\\npneumonia in the United States had risen sharply in 1915\\nand 1916 because of a major respiratory disease epidemic\\nbeginning in December 1915 (22). Death rates then dipped\\nslightly in 1917. The ﬁrst pandemic inﬂuenza wave\\nappeared in the spring of 1918, followed in rapid succes-\\nsion by much more fatal second and third waves in the fall\\nand winter of 191871919, respectively (Figure 1). Is it pos-\\nsible that a poorly-adapted H1N1 Virus was already begin-\\nning to spread in 1915, causing some serious illnesses but\\nnot yet sufﬁciently ﬁt to initiate a pandemic? Data consis-\\ntent with this possibility were reported at the time from\\nEuropean military camps (23), but a counter argument is\\nthat if a strain with a new hemagglutinin (HA) was caus-\\ning enough illness to affect the US national death rates\\nfrom pneumonia and inﬂuenza, it should have caused a\\npandemic sooner, and when it eventually did, in 1918,\\nmany people should have been immune or at least partial-\\nly immunoprotected. “Herald” events in 1915, 1916, and\\npossibly even in early 1918, if they occurred, would be dif-\\nﬁcult to identify.\\n\\nThe 1918 inﬂuenza pandemic had another unique fea-\\nture, the simultaneous (or nearly simultaneous) infection\\nof humans and swine. The Virus of the 1918 pandemic like-\\nly expressed an antigenically novel subtype to which most\\nhumans and swine were immunologically naive in 1918\\n(12,20). Recently published sequence and phylogenetic\\nanalyses suggest that the genes encoding the HA and neu-\\nraminidase (NA) surface proteins of the 1918 Virus were\\nderived from an avianlike inﬂuenza Virus shortly before\\nthe start of the pandemic and that the precursor Virus had\\nnot circulated widely in humans or swine in the few\\ndecades before (12,15, 24). More recent analyses of the\\nother gene segments of the Virus also support this conclu-\\nsion. Regression analyses of human and swine inﬂuenza\\nsequences obtained from 1930 to the present place the ini-\\ntial circulation of the 1918 precursor Virus in humans at\\napproximately 191571918 (20). Thus, the precursor was\\nprobably not circulating widely in humans until shortly\\nbefore 1918, nor did it appear to have jumped directly\\nfrom any species of bird studied to date (19). In summary,\\nits origin remains puzzling.\\n\\nWere the 3 Waves in 1918—1 919 Caused\\nby the Same Virus? If So, How and Why?\\nHistorical records since the 16th century suggest that\\nnew inﬂuenza pandemics may appear at any time of year,\\nnot necessarily in the familiar annual winter patterns of\\ninterpandemic years, presumably because newly shifted\\ninﬂuenza Viruses behave differently when they ﬁnd a uni-\\nversal or highly susceptible human population. Thereafter,\\nconfronted by the selection pressures of population immu-\\nnity, these pandemic Viruses begin to drift genetically and\\neventually settle into a pattern of annual epidemic recur-\\nrences caused by the drifted Virus variants.\\n\\nFigure 1. Three pandemic waves: weekly combined inﬂuenza and\\npneumonia mortality, United Kingdom, 1918—1919 (21).\\n\\nIn the 1918-1919 pandemic, a ﬁrst or spring wave\\nbegan in March 1918 and spread unevenly through the\\nUnited States, Europe, and possibly Asia over the next 6\\nmonths (Figure 1). Illness rates were high, but death rates\\nin most locales were not appreciably above normal. A sec-\\nond or fall wave spread globally from September to\\nNovember 1918 and was highly fatal. In many nations, a\\nthird wave occurred in early 1919 (21). Clinical similari-\\nties led contemporary observers to conclude initially that\\nthey were observing the same disease in the successive\\nwaves. The milder forms of illness in all 3 waves were\\nidentical and typical of inﬂuenza seen in the 1889 pandem-\\nic and in prior interpandemic years. In retrospect, even the\\nrapid progressions from uncomplicated inﬂuenza infec-\\ntions to fatal pneumonia, a hallmark of the 191871919 fall\\nand winter waves, had been noted in the relatively few\\nsevere spring wave cases. The differences between the\\nwaves thus seemed to be primarily in the much higher fre-\\nquency of complicated, severe, and fatal cases in the last 2\\nwaves.\\n\\nBut 3 extensive pandemic waves of inﬂuenza within 1\\nyear, occurring in rapid succession, with only the briefest\\nof quiescent intervals between them, was unprecedented.\\nThe occurrence, and to some extent the severity, of recur-\\nrent annual outbreaks, are driven by Viral antigenic drift,\\nwith an antigenic variant Virus emerging to become domi-\\nnant approximately every 2 to 3 years. Without such drift,\\ncirculating human inﬂuenza Viruses would presumably\\ndisappear once herd immunity had reached a critical\\nthreshold at which further Virus spread was sufﬁciently\\nlimited. The timing and spacing of inﬂuenza epidemics in\\ninterpandemic years have been subjects of speculation for\\ndecades. Factors believed to be responsible include partial\\nherd immunity limiting Virus spread in all but the most\\nfavorable circumstances, which include lower environ-\\nmental temperatures and human nasal temperatures (bene-\\nﬁcial to thermolabile Viruses such as inﬂuenza), optimal\\nhumidity, increased crowding indoors, and imperfect ven-\\ntilation due to closed windows and suboptimal airﬂow.\\n\\nHowever, such factors cannot explain the 3 pandemic\\nwaves of 1918-1919, which occurred in the spring-sum-\\nmer, summer—fall, and winter (of the Northern\\nHemisphere), respectively. The ﬁrst 2 waves occurred at a\\ntime of year normally unfavorable to inﬂuenza Virus\\nspread. The second wave caused simultaneous outbreaks\\nin the Northern and Southern Hemispheres from\\nSeptember to November. Furthermore, the interwave peri-\\nods were so brief as to be almost undetectable in some\\nlocales. Reconciling epidemiologically the steep drop in\\ncases in the ﬁrst and second waves with the sharp rises in\\ncases of the second and third waves is difﬁcult. Assuming\\neven transient postinfection immunity, how could suscep-\\ntible persons be too few to sustain transmission at 1 point,\\nand yet enough to start a new explosive pandemic wave a\\nfew weeks later? Could the Virus have mutated profoundly\\nand almost simultaneously around the world, in the short\\nperiods between the successive waves? Acquiring Viral\\ndrift sufﬁcient to produce new inﬂuenza strains capable of\\nescaping population immunity is believed to take years of\\nglobal circulation, not weeks of local circulation. And hav-\\ning occurred, such mutated Viruses normally take months\\nto spread around the world.\\n\\nAt the beginning of other “off season” inﬂuenza pan-\\ndemics, successive distinct waves within a year have not\\nbeen reported. The 1889 pandemic, for example, began in\\nthe late spring of 1889 and took several months to spread\\nthroughout the world, peaking in northern Europe and the\\nUnited States late in 1889 or early in 1890. The second\\nrecurrence peaked in late spring 1891 (more than a year\\nafter the ﬁrst pandemic appearance) and the third in early\\n1892 (21 ). As was true for the 1918 pandemic, the second\\n1891 recurrence produced of the most deaths. The 3 recur-\\nrences in 1889-1892, however, were spread over >3 years,\\nin contrast to 191871919, when the sequential waves seen\\nin individual countries were typically compressed into\\nz879 months.\\n\\nWhat gave the 1918 Virus the unprecedented ability to\\ngenerate rapidly successive pandemic waves is unclear.\\nBecause the only 1918 pandemic Virus samples we have\\nyet identiﬁed are from second-wave patients ([6), nothing\\ncan yet be said about whether the ﬁrst (spring) wave, or for\\nthat matter, the third wave, represented circulation of the\\nsame Virus or variants of it. Data from 1918 suggest that\\npersons infected in the second wave may have been pro-\\ntected from inﬂuenza in the third wave. But the few data\\nbearing on protection during the second and third waves\\nafter infection in the ﬁrst wave are inconclusive and do lit-\\ntle to resolve the question of whether the ﬁrst wave was\\ncaused by the same Virus or whether major genetic evolu-\\ntionary events were occurring even as the pandemic\\nexploded and progressed. Only inﬂuenza RNAipositive\\nhuman samples from before 1918, and from all 3 waves,\\ncan answer this question.\\n\\nWhat Was the Animal Host\\nOrigin of the Pandemic Virus?\\n\\nViral sequence data now suggest that the entire 1918\\nVirus was novel to humans in, or shortly before, 1918, and\\nthat it thus was not a reassortant Virus produced from old\\nexisting strains that acquired 1 or more new genes, such as\\nthose causing the 1957 and 1968 pandemics. On the con-\\ntrary, the 1918 Virus appears to be an avianlike inﬂuenza\\nVirus derived in toto from an unknown source (17,19), as\\nits 8 genome segments are substantially different from\\ncontemporary avian inﬂuenza genes. Inﬂuenza Virus gene\\nsequences from a number ofﬁxed specimens ofwild birds\\ncollected circa 1918 show little difference from avian\\nViruses isolated today, indicating that avian Viruses likely\\nundergo little antigenic change in their natural hosts even\\nover long periods (24,25).\\n\\nFor example, the 1918 nucleoprotein (NP) gene\\nsequence is similar to that ofviruses found in wild birds at\\nthe amino acid level but very divergent at the nucleotide\\nlevel, which suggests considerable evolutionary distance\\nbetween the sources of the 1918 NP and of currently\\nsequenced NP genes in wild bird strains (13,19). One way\\nof looking at the evolutionary distance of genes is to com-\\npare ratios of synonymous to nonsynonymous nucleotide\\nsubstitutions. A synonymous substitution represents a\\nsilent change, a nucleotide change in a codon that does not\\nresult in an amino acid replacement. A nonsynonymous\\nsubstitution is a nucleotide change in a codon that results\\nin an amino acid replacement. Generally, a Viral gene sub-\\njected to immunologic drift pressure or adapting to a new\\nhost exhibits a greater percentage of nonsynonymous\\nmutations, while a Virus under little selective pressure\\naccumulates mainly synonymous changes. Since little or\\nno selection pressure is exerted on synonymous changes,\\nthey are thought to reﬂect evolutionary distance.\\n\\nBecause the 1918 gene segments have more synony-\\nmous changes from known sequences of wild bird strains\\nthan expected, they are unlikely to have emerged directly\\nfrom an avian inﬂuenza Virus similar to those that have\\nbeen sequenced so far. This is especially apparent when\\none examines the differences at 4-fold degenerate codons,\\nthe subset of synonymous changes in which, at the third\\ncodon position, any of the 4 possible nucleotides can be\\nsubstituted without changing the resulting amino acid. At\\nthe same time, the 1918 sequences have too few amino acid\\ndiﬁerences from those of wild-bird strains to have spent\\nmany years adapting only in a human or swine intermedi-\\nate host. One possible explanation is that these unusual\\ngene segments were acquired from a reservoir of inﬂuenza\\nVirus that has not yet been identiﬁed or sampled. All of\\nthese ﬁndings beg the question: where did the 1918 Virus\\ncome from?\\n\\nIn contrast to the genetic makeup of the 1918 pandem-\\nic Virus, the novel gene segments of the reassorted 1957\\nand 1968 pandemic Viruses all originated in Eurasian avian\\nViruses (26); both human Viruses arose by the same mech-\\nanismireassortment of a Eurasian wild waterfowl strain\\nwith the previously circulating human H1N1 strain.\\nProving the hypothesis that the Virus responsible for the\\n1918 pandemic had a markedly different origin requires\\nsamples of human inﬂuenza strains circulating before\\n1918 and samples of inﬂuenza strains in the wild that more\\nclosely resemble the 1918 sequences.\\n\\nWhat Was the Biological Basis for\\n1918 Pandemic Virus Pathogenicity?\\n\\nSequence analysis alone does not oﬁer clues to the\\npathogenicity of the 1918 Virus. A series of experiments\\nare under way to model Virulence in Vitro and in animal\\nmodels by using Viral constructs containing 1918 genes\\nproduced by reverse genetics.\\n\\nInﬂuenza Virus infection requires binding of the HA\\nprotein to sialic acid receptors on host cell surface. The HA\\nreceptor-binding site conﬁguration is different for those\\ninﬂuenza Viruses adapted to infect birds and those adapted\\nto infect humans. Inﬂuenza Virus strains adapted to birds\\npreferentially bind sialic acid receptors with 01 (273) linked\\nsugars (27729). Human-adapted inﬂuenza Viruses are\\nthought to preferentially bind receptors with 01 (2%) link-\\nages. The switch from this avian receptor conﬁguration\\nrequires of the Virus only 1 amino acid change (30), and\\nthe HAs of all 5 sequenced 1918 Viruses have this change,\\nwhich suggests that it could be a critical step in human host\\nadaptation. A second change that greatly augments Virus\\nbinding to the human receptor may also occur, but only 3\\nof5 1918 HA sequences have it (16).\\n\\nThis means that at least 2 H1N1 receptor-binding vari-\\nants cocirculated in 1918: 1 with high—afﬁnity binding to\\nthe human receptor and 1 with mixed-afﬁnity binding to\\nboth avian and human receptors. No geographic or chrono-\\nlogic indication eXists to suggest that one of these variants\\nwas the precursor of the other, nor are there consistent dif-\\nferences between the case histories or histopathologic fea-\\ntures of the 5 patients infected with them. Whether the\\nViruses were equally transmissible in 1918, whether they\\nhad identical patterns of replication in the respiratory tree,\\nand whether one or both also circulated in the ﬁrst and\\nthird pandemic waves, are unknown.\\nIn a series of in Vivo experiments, recombinant inﬂuen-\\nza Viruses containing between 1 and 5 gene segments of\\nthe 1918 Virus have been produced. Those constructs\\nbearing the 1918 HA and NA are all highly pathogenic in\\n\\nmice (31). Furthermore, expression microarray analysis\\nperformed on whole lung tissue of mice infected with the\\n1918 HA/NA recombinant showed increased upregulation\\nof genes involved in apoptosis, tissue injury, and oxidative\\ndamage (32). These ﬁndings are unexpected because the\\nViruses with the 1918 genes had not been adapted to mice;\\ncontrol experiments in which mice were infected with\\nmodern human Viruses showed little disease and limited\\nViral replication. The lungs of animals infected with the\\n1918 HA/NA construct showed bronchial and alveolar\\nepithelial necrosis and a marked inﬂammatory inﬁltrate,\\nwhich suggests that the 1918 HA (and possibly the NA)\\ncontain Virulence factors for mice. The Viral genotypic\\nbasis of this pathogenicity is not yet mapped. Whether\\npathogenicity in mice effectively models pathogenicity in\\nhumans is unclear. The potential role of the other 1918 pro-\\nteins, singularly and in combination, is also unknown.\\nExperiments to map further the genetic basis of Virulence\\nof the 1918 Virus in various animal models are planned.\\nThese experiments may help deﬁne the Viral component to\\nthe unusual pathogenicity of the 1918 Virus but cannot\\naddress whether speciﬁc host factors in 1918 accounted for\\nunique inﬂuenza mortality patterns.\\n\\nWhy Did the 1918 Virus Kill So Many Healthy\\nYoung Ad ults?\\n\\nThe curve of inﬂuenza deaths by age at death has histor-\\nically, for at least 150 years, been U-shaped (Figure 2),\\nexhibiting mortality peaks in the very young and the very\\nold, with a comparatively low frequency of deaths at all\\nages in between. In contrast, age-speciﬁc death rates in the\\n1918 pandemic exhibited a distinct pattern that has not been\\ndocumented before or since: a “W—shaped” curve, similar to\\nthe familiar U-shaped curve but with the addition of a third\\n(middle) distinct peak of deaths in young adults z20410\\nyears of age. Inﬂuenza and pneumonia death rates for those\\n1534 years of age in 191871919, for example, were\\n20 times higher than in previous years (35). Overall, near-\\nly half of the inﬂuenza—related deaths in the 1918 pandem-\\nic were in young adults 20410 years of age, a phenomenon\\nunique to that pandemic year. The 1918 pandemic is also\\nunique among inﬂuenza pandemics in that absolute risk of\\ninﬂuenza death was higher in those <65 years of age than in\\nthose >65; persons <65 years of age accounted for >99% of\\nall excess inﬂuenza—related deaths in 191871919. In com-\\nparison, the <65-year age group accounted for 36% of all\\nexcess inﬂuenza—related deaths in the 1957 H2N2 pandem-\\nic and 48% in the 1968 H3N2 pandemic (33).\\nA sharper perspective emerges when 1918 age-speciﬁc\\ninﬂuenza morbidity rates (21) are used to adj ust the W-\\nshaped mortality curve (Figure 3, panels, A, B, and C\\n[35,37]). Persons 65 years of age in 1918 had a dispro-\\nportionately high inﬂuenza incidence (Figure 3, panel A).\\n\\nBut even after adjusting age-speciﬁc deaths by age-specif—\\nic clinical attack rates (Figure 3, panel B), a W—shaped\\ncurve with a case-fatality peak in young adults remains and\\nis signiﬁcantly different from U-shaped age-speciﬁc case-\\nfatality curves typically seen in other inﬂuenza years, e.g.,\\n192871929 (Figure 3, panel C). Also, in 1918 those 5 to 14\\nyears of age accounted for a disproportionate number of\\ninﬂuenza cases, but had a much lower death rate from\\ninﬂuenza and pneumonia than other age groups. To explain\\nthis pattern, we must look beyond properties of the Virus to\\nhost and environmental factors, possibly including\\nimmunopathology (e.g., antibody-dependent infection\\nenhancement associated with prior Virus exposures [38])\\nand exposure to risk cofactors such as coinfecting agents,\\nmedications, and environmental agents.\\n\\nOne theory that may partially explain these ﬁndings is\\nthat the 1918 Virus had an intrinsically high Virulence, tem-\\npered only in those patients who had been born before\\n1889, e.g., because of exposure to a then-circulating Virus\\ncapable of providing partial immunoprotection against the\\n1918 Virus strain only in persons old enough (>35 years) to\\nhave been infected during that prior era (35). But this the-\\nory would present an additional paradox: an obscure pre-\\ncursor Virus that left no detectable trace today would have\\nhad to have appeared and disappeared before 1889 and\\nthen reappeared more than 3 decades later.\\n\\nEpidemiologic data on rates of clinical inﬂuenza by\\nage, collected between 1900 and 1918, provide good evi-\\ndence for the emergence of an antigenically novel inﬂuen-\\nza Virus in 1918 (21). Jordan showed that from 1900 to\\n1917, the 5- to 15-year age group accounted for 11% of\\ntotal inﬂuenza cases, while the >65-year age group\\naccounted for 6 % of inﬂuenza cases. But in 1918, cases in\\n\\nFigure 2. “U-” and “W—” shaped combined inﬂuenza and pneumo-\\nnia mortality, by age at death, per 100,000 persons in each age\\ngroup, United States, 1911—1918. Influenza- and pneumonia-\\nspeciﬁc death rates are plotted for the interpandemic years\\n1911—1917 (dashed line) and for the pandemic year 1918 (solid\\nline) (33,34).\\n\\nIncidence male per 1 .nao persunslage group\\nMortality per 1.000 persunslige group\\n\\n+ Case—fataiity rale 1918—1919 \\n\\nCase fatalily par 100 persons ill wilh P&I pel age group\\n\\nFigure 3. Influenza plus pneumonia (P&l) (combined) age-specific\\nincidence rates per 1,000 persons per age group (panel A), death\\nrates per 1,000 persons, ill and well combined (panel B), and\\ncase-fatality rates (panel C, solid line), US Public Health Service\\nhouse-to-house surveys, 8 states, 1918 (36). A more typical curve\\nof age-specific influenza case-fatality (panel C, dotted line) is\\ntaken from US Public Health Service surveys during 1928—1929\\n(37).\\n\\nthe 5 to 15-year-old group jumped to 25% of inﬂuenza\\ncases (compatible with exposure to an antigenically novel\\nVirus strain), while the >65-year age group only accounted\\nfor 0.6% of the inﬂuenza cases, ﬁndings consistent with\\npreviously acquired protective immunity caused by an\\nidentical or closely related Viral protein to which older per-\\nsons had once been exposed. Mortality data are in accord.\\nIn 1918, persons >75 years had lower inﬂuenza and\\n\\npneumonia case-fatality rates than they had during the\\nprepandemic period of 191171917. At the other end of the\\nage spectrum (Figure 2), a high proportion of deaths in\\ninfancy and early childhood in 1918 mimics the age pat-\\ntern, if not the mortality rate, of other inﬂuenza pandemics.\\n\\nCould a 1918-like Pandemic Appear Again?\\nIf So, What Could We Do About It?\\n\\nIn its disease course and pathologic features, the 1918\\npandemic was different in degree, but not in kind, from\\nprevious and subsequent pandemics. Despite the extraordi-\\nnary number of global deaths, most inﬂuenza cases in\\n1918 (>95% in most locales in industrialized nations) were\\nmild and essentially indistinguishable from inﬂuenza cases\\ntoday. Furthermore, laboratory experiments with recombi-\\nnant inﬂuenza Viruses containing genes from the 1918\\nVirus suggest that the 1918 and 1918-like Viruses would be\\nas sensitive as other typical Virus strains to the Food and\\nDrug Administrationiapproved antiinﬂuenza drugs riman-\\ntadine and oseltamivir.\\n\\nHowever, some characteristics of the 1918 pandemic\\nappear unique: most notably, death rates were 5 7 20 times\\nhigher than expected. Clinically and pathologically, these\\nhigh death rates appear to be the result of several factors,\\nincluding a higher proportion of severe and complicated\\ninfections of the respiratory tract, rather than involvement\\nof organ systems outside the normal range of the inﬂuenza\\nVirus. Also, the deaths were concentrated in an unusually\\nyoung age group. Finally, in 1918, 3 separate recurrences\\nof inﬂuenza followed each other with unusual rapidity,\\nresulting in 3 explosive pandemic waves within a year’s\\ntime (Figure 1). Each of these unique characteristics may\\nreﬂect genetic features of the 1918 Virus, but understand-\\ning them will also require examination of host and envi-\\nronmental factors.\\n\\nUntil we can ascertain which of these factors gave rise\\nto the mortality patterns observed and learn more about the\\nformation of the pandemic, predictions are only educated\\nguesses. We can only conclude that since it happened once,\\nanalogous conditions could lead to an equally devastating\\npandemic.\\n\\nLike the 1918 Virus, H5N1 is an avian Virus (39),\\nthough a distantly related one. The evolutionary path that\\nled to pandemic emergence in 1918 is entirely unknown,\\nbut it appears to be different in many respects from the cur-\\nrent situation with H5N1. There are no historical data,\\neither in 1918 or in any other pandemic, for establishing\\nthat a pandemic “precursor” Virus caused a highly patho-\\ngenic outbreak in domestic poultry, and no highly patho-\\ngenic avian inﬂuenza (HPAI) Virus, including H5N1 and a\\nnumber of others, has ever been known to cause a major\\nhuman epidemic, let alone a pandemic. While data bearing\\non inﬂuenza Virus human cell adaptation (e.g., receptor\\nbinding) are beginning to be understood at the molecular\\nlevel, the basis for Viral adaptation to efﬁcient human-to-\\nhuman spread, the chief prerequisite for pandemic emer-\\ngence, is unknown for any inﬂuenza Virus. The 1918 Virus\\nacquired this trait, but we do not know how, and we cur-\\nrently have no way of knowing whether H5N1 Viruses are\\nnow in a parallel process of acquiring human-to-human\\ntransmissibility. Despite an explosion of data on the 1918\\nVirus during the past decade, we are not much closer to\\nunderstanding pandemic emergence in 2006 than we were\\nin understanding the risk of H1N1 “swine ﬂu” emergence\\nin 1976.\\n\\nEven with modern antiviral and antibacterial drugs,\\nvaccines, and prevention knowledge, the return of a pan-\\ndemic Virus equivalent in pathogenicity to the Virus of\\n1918 would likely kill >100 million people worldwide. A\\npandemic Virus with the (alleged) pathogenic potential of\\nsome recent H5N1 outbreaks could cause substantially\\nmore deaths.\\n\\nWhether because of Viral, host or environmental fac-\\ntors, the 1918 Virus causing the ﬁrst or ‘spring’ wave was\\nnot associated with the exceptional pathogenicity of the\\nsecond (fall) and third (winter) waves. Identiﬁcation of an\\ninﬂuenza RNA-positive case from the ﬁrst wave could\\npoint to a genetic basis for Virulence by allowing differ-\\nences in Viral sequences to be highlighted. Identiﬁcation of\\npre-1918 human inﬂuenza RNA samples would help us\\nunderstand the timing of emergence of the 1918 Virus.\\nSurveillance and genomic sequencing of large numbers of\\nanimal inﬂuenza Viruses will help us understand the genet-\\nic basis of host adaptation and the extent of the natural\\nreservoir of inﬂuenza Viruses. Understanding inﬂuenza\\npandemics in general requires understanding the 1918 pan-\\ndemic in all its historical, epidemiologic, and biologic\\naspects.\\n\\nDr Taubenberger is chair of the Department of Molecular\\nPathology at the Armed Forces Institute of Pathology, Rockville,\\nMaryland. His research interests include the molecular patho-\\nphysiology and evolution of inﬂuenza Viruses.\\n\\nDr Morens is an epidemiologist with a long-standing inter-\\nest in emerging infectious diseases, Virology, tropical medicine,\\nand medical history. Since 1999, he has worked at the National\\nInstitute of Allergy and Infectious Diseases.\\n\\nReferences\\n\\n1. Frost WH. Statistics of inﬂuenza morbidity. Public Health Rep.\\n19203558497.\\n2. Bumet F, Clark E. Inﬂuenza: a survey ofthe last 50 years in the light\\nof modern work on the Virus of epidemic inﬂuenza. Melbourne:\\nMacMillan; 1942.\\n3. Marks G, Beatty WK. Epidemics. New York: Scribners, 1976.\\n4. Rosenau MJ, Last JM. Maxcy-Rosenau preventative medicine and\\npublic health. New York: Appleton-Century-Crofts; 1980.\\n5. Crosby A. America’s forgotten pandemic. Cambridge (UK):\\nCambridge University Press;1989.\\n6. Patterson KD, Pyle GF. The geography and mortality of the 1918\\ninfluenza pandemic. Bull Hist Med. 1991;65:4–21.\\n7. Johnson NPAS, Mueller J. Updating the accounts: global mortality of\\nthe 1918–1920 “Spanish” influenza pandemic. Bull Hist Med\\n2002;76:105–15.\\n8. Shope RE. The incidence of neutralizing antibodies for swine\\ninfluenza virus in the sera of human beings of different ages. J Exp\\nMed. 1936;63:669–84.\\n9. Kendal AP, Noble GR, Skehel JJ, Dowdle WR. Antigenic similarity\\nof influenza A (H1N1) viruses from epidemics in 1977–1978 to\\n“Scandinavian” strains isolated in epidemics of 1950–1951. Virology.\\n1978;89:632–6.\\n10. Taubenberger JK, Reid AH, Krafft AE, Bijwaard KE, Fanning TG.\\nInitial genetic characterization of the 1918 “Spanish” influenza virus.\\nScience. 1997;275:1793–6.\\n11. Basler CF, Reid AH, Dybing JK, Janczewski TA, Fanning TG, Zheng\\nH, et al. Sequence of the 1918 pandemic influenza virus nonstructural gene (NS) segment and characterization of recombinant viruses\\nbearing the 1918 NS genes. Proc Natl Acad Sci U S A\\n2001;98:2746–51.\\n12. Reid AH, Fanning TG, Hultin JV, Taubenberger JK. Origin and evolution of the 1918 “Spanish” influenza virus hemagglutinin gene.\\nProc Natl Acad Sci U S A 1999;96:1651–6.\\n13. Reid AH, Fanning TG, Janczewski TA, Lourens RM, and\\nTaubenberger JK. Novel origin of the 1918 pandemic influenza virus\\nnucleoprotein gene segment. J Virol. 2004;78:12462–70.\\n14. Reid AH, Fanning TG, Janczewski TA, McCall S, Taubenberger JK.\\nCharacterization of the 1918 “Spanish” influenza virus matrix gene\\nsegment. J Virol. 2002;76:10717–23.\\n15. Reid AH, Fanning TG, Janczewski TA, Taubenberger JK.\\nCharacterization of the 1918 “Spanish” influenza virus neuraminidase gene. Proc Natl Acad Sci U S A 2000;97:6785–90.\\n16. Reid AH, Janczewski TA, Lourens RM, Elliot AJ, Daniels RS, Berry\\nCL, et al. 1918 influenza pandemic caused by highly conserved viruses with two receptor-binding variants. Emerg Infect Dis.\\n2003;9:1249–53.\\n17. Taubenberger JK, Reid AH, Lourens RM, Wang R, Jin G, Fanning\\nTG. Characterization of the 1918 influenza virus polymerase genes.\\nNature. 2005;437:889–93.\\n18. Reid AH, Taubenberger JK. The 1918 flu and other influenza pandemics: “over there” and back again. Lab Invest. 1999;79:95–101.\\n19. Reid AH, Taubenberger JK, Fanning TG. Evidence of an absence: the\\ngenetic origins of the 1918 pandemic influenza virus. Nat Rev\\nMicrobiol. 2004;2:909–14.\\n20. Taubenberger JK, Reid AH, Fanning TG. The 1918 influenza virus: a\\nkiller comes into view. Virology. 2000;274:241–5.\\n21. Jordan E. Epidemic influenza: a survey. Chicago: American Medical\\nAssociation, 1927.\\n22. Capps J, Moody A. The recent epidemic of grip. JAMA.\\n1916;67:1349–50.\\n33. Oxford JS, Sefton A, Jackson R, Innes W, Daniels RS, Johnson NP.\\nWorld War I may have allowed the emergence of “Spanish” influenza. Lancet Infect Dis. 2002;2:111–4.\\n24. Fanning TG, Slemons RD, Reid AH, Janczewski TA, Dean J,\\nTaubenberger JK. 1917 avian influenza virus sequences suggest that\\nthe 1918 pandemic virus did not acquire its hemagglutinin directly\\nfrom birds. J Virol. 2002;76:7860–2.\\n25. Reid AH, Fanning TG, Slemons RD, Janczewski TA, Dean J,\\nTaubenberger JK. Relationship of pre-1918 avian influenza HA and\\nNP sequences to subsequent avian influenza strains. Avian Dis.\\n2003;47:921–5.\\n26. Bean W, Schell M, Katz J, Kawaoka Y, Naeve C, Gorman O, et al.\\nEvolution of the H3 influenza virus hemagglutinin from human and\\nnonhuman hosts. J Virol. 1992;66:1129–38.\\n27. Weis W, Brown JH, Cusack S, Paulson JC, Skehel JJ, Wiley DC.\\nStructure of the influenza virus haemagglutinin complexed with its\\nreceptor, sialic acid. Nature. 1988;333:426–31.\\n28. Gambaryan AS, Tuzikov AB, Piskarev VE, Yamnikova SS, Lvov DK,\\nRobertson JS, et al. Specification of receptor-binding phenotypes of\\ninfluenza virus isolates from different hosts using synthetic sialylglycopolymers: non-egg-adapted human H1 and H3 influenza A and\\ninfluenza B viruses share a common high binding affinity for 6′-sialyl(N-acetyllactosamine). Virology. 1997;232: 345–50.\\n29. Matrosovich M, Gambaryan A, Teneberg S, Piskarev VE, Yamnikova\\nSS, Lvov DK, et al. Avian influenza A viruses differ from human\\nviruses by recognition of sialyloigosaccharides and gangliosides and\\nby a higher conservation of the HA receptor-binding site. Virology.\\n1997;233:224–34.\\n30. Glaser L, Stevens J, Zamarin D, Wilson IA, Garcia-Sastre A, Tumpey\\nTM, et al. A single amino acid substitution in the 1918 influenza virus\\nhemagglutinin changes the receptor binding specificity. J Virol.\\n2005;79:11533–6.\\n31. Kobasa D, Takada A, Shinya K, Hatta M, Halfmann P, Theriault S, et\\nal. Enhanced virulence of influenza A viruses with the haemagglutinin of the 1918 pandemic virus. Nature. 2004;431:703–7.\\n32. Kash JC, Basler CF, Garcia-Sastre A, Carter V, Billharz R, Swayne\\nDE, et al. Global host immune response: pathogenesis and transcriptional profiling of type A influenza viruses expressing the hemagglutinin and neuraminidase genes from the 1918 pandemic virus. J Virol.\\n2004;78:9499–511.\\n33. Grove RD, Hetzel AM. Vital statistics rates in the United States:\\n1940–1960. Washington: US Government Printing Office, 1968.\\n34. Linder FE, Grove RD. Vital statistics rates in the United States:\\n1900–1940. Washington: US Government Printing Office, 1943.\\n35. Simonsen L, Clarke MJ, Schonberger LB, Arden NH, Cox NJ,\\nFukuda K. Pandemic versus epidemic influenza mortality: a pattern\\nof changing age distribution. J Infect Dis 1998;178:53–60.\\n36. Frost WH. The epidemiology of influenza. Public Health Rep.\\n1919;34:1823–61.\\n37. Collins SD. Age and sex incidence of influenza and pneumonia morbidity and mortality in the epidemic of 1928-1929 with comparative\\ndata for the epidemic of 1918–1919. Public Health Rep.\\n1931;46:1909–37.\\n38. Majde JA. Influenza: Learn from the past. ASM News. 1996;62:514.\\n39. Peiris JS, Yu WC, Leung CW, Cheung CY, Ng WF, Nicholls JM, et al.\\nRe-emergence of fatal human influenza A subtype H5N1 disease.\\nLancet. 2004;363:617–9.\\n\\nAddress for correspondence: Jeffery K. Taubenberger, Department of\\nMolecular Pathology, Armed Forces Institute of Pathology, 1413\\nResearch Blvd, Bldg 101, Rm 1057, Rockville, MD 20850-3125, USA;\\nfax. 301-295-9507; email: taubenberger@afip.osd.mil\\n\\nThe opinions expressed by authors contributing to this journal do\\nnot necessarily reflect the opinions of the Centers for Disease\\nControl and Prevention or the institutions with which the authors\\nare affiliated.',\n",
       "   'document_id': 2684}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6d745-e12e-4681-8d19-20854d76633d",
   "metadata": {},
   "source": [
    "# mlm prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7879f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.squad.train import BertForQA\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ec3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040e5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e663bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = RobertaForMaskedLM.from_pretrained('./cached_models/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf4eca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:335: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  tags = yaml.load(fp)\n",
      "Some weights of the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base were not used when initializing BertForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'qa_classifier.weight', 'qa_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQA.load_from_checkpoint(\n",
    "        checkpoint_path=\"./cached_models/roberta_squad1_2epoch/roberta_squad1_2epoch.ckpt\",\n",
    "        hparams_file=\"./outputs/roberta-base/squad1/lightning_logs/squad1_squad1/hparams.yaml\",\n",
    "        map_location=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "120cdbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vocabulary path (True) should be a directory\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cddcb050f9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./cached_models/roberta_mlm/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_save_pretrained\u001b[0;34m(self, save_directory, file_names, legacy_format, filename_prefix)\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1950\u001b[0;31m         \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1951\u001b[0m         \u001b[0mlegacy_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "model.tokenizer._save_pretrained(True,\"./cached_models/roberta_mlm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145b02b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model.roberta.load_state_dict(model.model.roberta.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f637c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model.save_pretrained(\"./cached_models/roberta_mlm/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56c8d7",
   "metadata": {},
   "source": [
    "# Convert MLM to Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034a5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tasks.squad.train import BertForQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2314c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb8b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = RobertaForMaskedLM.from_pretrained('./cached_models/roberta_squad1_2epoch_covidmlm_3epoch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0899ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base were not used when initializing BertForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'qa_classifier.weight', 'qa_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQA.load_from_checkpoint(\n",
    "        checkpoint_path=\"./cached_models/roberta_squad1_2epoch/roberta_squad1_2epoch.ckpt\",\n",
    "        hparams_file=\"./outputs/roberta-base/squad1/lightning_logs/squad1_squad1/hparams.yaml\",\n",
    "        map_location=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39703b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "INFO:lightning:GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6064f109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.roberta.load_state_dict(mlm_model.roberta.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8c4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./cached_models/roberta_squad1_2epoch_covidmlm_3epoch/roberta_squad1_2epoch_covidmlm_3epoch.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabb9bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQA(\n",
       "  (model): BertForQuestionAnswering(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (qa_classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f432e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "895ece2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "INFO:lightning:GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "/opt/anaconda/envs/covidqa/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(default_root_dir=\"./cached_models/roberta_squad1_2epoch/roberta_squad1_2epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2211f693",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-862a6c3653f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "trainer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded5031",
   "metadata": {},
   "source": [
    "# Convert Text Classification To QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15fcc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tasks.tnews.train import TNewsClassificationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6bc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6b6265",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_state_dict() missing 2 required positional arguments: 'self' and 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b67c975ccbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_state_dict() missing 2 required positional arguments: 'self' and 'state_dict'"
     ]
    }
   ],
   "source": [
    "RobertaForSequenceClassification.from_pre_trained_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8885d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta_squad1_2epoch were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta_squad1_2epoch and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cls_model = TNewsClassificationTask.load_from_checkpoint(\n",
    "        checkpoint_path=\"./outputs/roberta_squad1_2epoch/covidq_cls/epoch=3_v0.ckpt\",\n",
    "        hparams_file=\"outputs/roberta_squad1_2epoch/covidq_cls/lightning_logs/version_0/hparams.yaml\",\n",
    "        map_location=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ad2881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.roberta.save_pre_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d63210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base were not used when initializing BertForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/xiangpan/Labs/CovidQA/multi_task_qa/cached_models/roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'qa_classifier.weight', 'qa_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQA.load_from_checkpoint(\n",
    "        checkpoint_path=\"./cached_models/roberta_squad1_2epoch/roberta_squad1_2epoch.ckpt\",\n",
    "        hparams_file=\"./outputs/roberta-base/squad1/lightning_logs/squad1_squad1/hparams.yaml\",\n",
    "        map_location=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60cd0c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.roberta.load_state_dict(cls_model.model.roberta.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefce5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./cached_models/roberta_squad1_2epoch_covidqcls_3epoch/roberta_squad1_2epoch_covidqcls_3epoch.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c7875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
